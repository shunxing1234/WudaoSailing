[2022-01-18 01:47:28,431] [INFO] [runner.py:327:main] Using IP address of 172.31.32.40 for node V100-3
[2022-01-18 01:47:28,432] [INFO] [runner.py:398:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJWMTAwLTMiOiBbMCwgMSwgMl19 --master_addr=172.31.32.40 --master_port=55131 pretrain_glm.py --block-lm --task-mask --bert-prob 0.4 --gap-sentence-prob 0.3 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-large-chinese --model-parallel-size 1 --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --seq-length 512 --max-position-embeddings 1024 --save ../model_save/checkpoints/ --load ../model_save/checkpoints/ --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000000 --train-data wudao --resume-dataloader --loader-scatter 4 --no-lazy-loader --tokenizer-type ChineseSPTokenizer --fix-command-token --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 200000 --warmup 0.04 --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config ./config/config_block_large_chinese.json
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NCCL_DEBUG=info
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL=2
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE=0
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda10.2
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NCCL_INCLUDE_DIR=/usr/include
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1
[2022-01-18 01:47:29,170] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_VERSION=2.11.4-1
[2022-01-18 01:47:29,171] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1
[2022-01-18 01:47:29,171] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda10.2
[2022-01-18 01:47:29,171] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2022-01-18 01:47:29,171] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2022-01-18 01:47:29,171] [INFO] [launch.py:73:main] 0 NCCL_LIBRARY=/usr/lib/x86_64-linux-gnu
[2022-01-18 01:47:29,171] [INFO] [launch.py:80:main] WORLD INFO DICT: {'V100-3': [0, 1, 2]}
[2022-01-18 01:47:29,171] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=3, node_rank=0
[2022-01-18 01:47:29,171] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'V100-3': [0, 1, 2]})
[2022-01-18 01:47:29,171] [INFO] [launch.py:100:main] dist_world_size=3
[2022-01-18 01:47:29,171] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0,1,2
using world size: 3 and model-parallel size: 1 
 > using dynamic loss scaling
> initializing model parallel with size 1
[2022-01-18 01:47:31,344] [INFO] [checkpointing.py:795:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}
[2022-01-18 01:47:31,344] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
{'pad': 50000, 'eos': 50000, 'sep': 50001, 'ENC': 50002, 'MASK': 50003, 'unk': 50004, 'sop': 50006, 'eop': 50007, 'sMASK': 50008, 'gMASK': 50009, 'dBLOCK': 50010}
> padded vocab (size: 50011) with 37 dummy tokens (new size: 50048)
> found end-of-document token: 50000
glm_dist16:16126:16126 [0] NCCL INFO Bootstrap : Using bond1:172.31.32.40<0>
glm_dist16:16126:16126 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
glm_dist16:16126:16126 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
glm_dist16:16126:16126 [0] NCCL INFO NET/IB : No device found.
glm_dist16:16126:16126 [0] NCCL INFO NET/Socket : Using [0]bond1:172.31.32.40<0> [1]vethf76a3c0:fe80::1c70:26ff:fe30:6d88%vethf76a3c0<0> [2]vethe25bc53:fe80::7c24:adff:fe52:b43d%vethe25bc53<0> [3]veth7a86593:fe80::5879:ffff:fe90:7190%veth7a86593<0> [4]vethbdea447:fe80::28d4:31ff:fe41:c788%vethbdea447<0>
glm_dist16:16126:16126 [0] NCCL INFO Using network Socket
NCCL version 2.11.4+cuda10.2
glm_dist16:16128:16128 [2] NCCL INFO Bootstrap : Using bond1:172.31.32.40<0>
glm_dist16:16128:16128 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
glm_dist16:16128:16128 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
glm_dist16:16128:16128 [2] NCCL INFO NET/IB : No device found.
glm_dist16:16128:16128 [2] NCCL INFO NET/Socket : Using [0]bond1:172.31.32.40<0> [1]vethf76a3c0:fe80::1c70:26ff:fe30:6d88%vethf76a3c0<0> [2]vethe25bc53:fe80::7c24:adff:fe52:b43d%vethe25bc53<0> [3]veth7a86593:fe80::5879:ffff:fe90:7190%veth7a86593<0> [4]vethbdea447:fe80::28d4:31ff:fe41:c788%vethbdea447<0>
glm_dist16:16128:16128 [2] NCCL INFO Using network Socket
NCCL version 2.11.4+cuda10.2
glm_dist16:16126:16152 [0] NCCL INFO Channel 00/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 01/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 02/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 03/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 04/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 05/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 06/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 07/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 08/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 09/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 10/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 11/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 12/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 13/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 14/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 15/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 16/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 17/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 18/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 19/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 20/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 21/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 22/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 23/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 24/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 25/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 26/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 27/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 28/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 29/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 30/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Channel 31/32 :    0
glm_dist16:16126:16152 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
glm_dist16:16126:16152 [0] NCCL INFO Setting affinity for GPU 0 to 3ffff0,0003ffff
glm_dist16:16127:16127 [1] NCCL INFO Bootstrap : Using bond1:172.31.32.40<0>
glm_dist16:16127:16127 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
glm_dist16:16127:16127 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
glm_dist16:16127:16127 [1] NCCL INFO NET/IB : No device found.
glm_dist16:16127:16127 [1] NCCL INFO NET/Socket : Using [0]bond1:172.31.32.40<0> [1]vethf76a3c0:fe80::1c70:26ff:fe30:6d88%vethf76a3c0<0> [2]vethe25bc53:fe80::7c24:adff:fe52:b43d%vethe25bc53<0> [3]veth7a86593:fe80::5879:ffff:fe90:7190%veth7a86593<0> [4]vethbdea447:fe80::28d4:31ff:fe41:c788%vethbdea447<0>
glm_dist16:16127:16127 [1] NCCL INFO Using network Socket
NCCL version 2.11.4+cuda10.2
glm_dist16:16126:16152 [0] NCCL INFO Connected all rings
glm_dist16:16126:16152 [0] NCCL INFO Connected all trees
glm_dist16:16126:16152 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
glm_dist16:16126:16152 [0] NCCL INFO comm 0x7f1320006a20 rank 0 nranks 1 cudaDev 0 busId 1b000 - Init COMPLETE
configuring data
Creating lazy loader for dataset wudao
Traceback (most recent call last):
  File "pretrain_glm.py", line 163, in <module>
    main()
  File "pretrain_glm.py", line 70, in main
    train_data, val_data, test_data, = get_train_val_test_data(args, tokenizer)
  File "/data/wang/models/wudao/data_utils/train_data.py", line 209, in get_train_val_test_data
    train_data, val_data, test_data = data_config.apply(args, tokenizer)
  File "/data/wang/models/wudao/data_utils/configure_data.py", line 107, in apply
    return make_loaders(args, tokenizer)
  File "/data/wang/models/wudao/data_utils/configure_data.py", line 308, in make_loaders
    train = data_utils.make_dataset(**data_set_args)
  File "/data/wang/models/wudao/data_utils/__init__.py", line 180, in make_dataset
    ds = [get_dataset(p, tokenizer=tokenizer, pre_tokenize=pre_tokenize, no_lazy_loader=no_lazy_loader,
  File "/data/wang/models/wudao/data_utils/__init__.py", line 180, in <listcomp>
    ds = [get_dataset(p, tokenizer=tokenizer, pre_tokenize=pre_tokenize, no_lazy_loader=no_lazy_loader,
  File "/data/wang/models/wudao/data_utils/__init__.py", line 74, in get_dataset
    reader = dataset(writers=writers, tokenizer=tokenizer, tokenize=pre_tokenize)
  File "/data/wang/models/wudao/data_utils/corpora.py", line 109, in __init__
    assert os.path.exists(self.PATH), self.assert_str
AssertionError: None
glm_dist16:16128:16155 [2] NCCL INFO Channel 00/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 01/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 02/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 03/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 04/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 05/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 06/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 07/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 08/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 09/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 10/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 11/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 12/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 13/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 14/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 15/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 16/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 17/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 18/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 19/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 20/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 21/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 22/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 23/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 24/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 25/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 26/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 27/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 28/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 29/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 30/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Channel 31/32 :    0
glm_dist16:16128:16155 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
glm_dist16:16128:16155 [2] NCCL INFO Setting affinity for GPU 2 to 3ffff0,0003ffff
glm_dist16:16127:16158 [1] NCCL INFO Channel 00/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 01/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 02/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 03/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 04/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 05/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 06/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 07/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 08/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 09/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 10/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 11/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 12/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 13/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 14/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 15/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 16/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 17/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 18/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 19/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 20/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 21/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 22/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 23/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 24/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 25/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 26/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 27/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 28/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 29/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 30/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Channel 31/32 :    0
glm_dist16:16127:16158 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
glm_dist16:16127:16158 [1] NCCL INFO Setting affinity for GPU 1 to 3ffff0,0003ffff
glm_dist16:16128:16155 [2] NCCL INFO Connected all rings
glm_dist16:16128:16155 [2] NCCL INFO Connected all trees
glm_dist16:16128:16155 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
glm_dist16:16128:16155 [2] NCCL INFO comm 0x7f1934006a20 rank 0 nranks 1 cudaDev 2 busId 1d000 - Init COMPLETE
glm_dist16:16127:16158 [1] NCCL INFO Connected all rings
glm_dist16:16127:16158 [1] NCCL INFO Connected all trees
glm_dist16:16127:16158 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
glm_dist16:16127:16158 [1] NCCL INFO comm 0x7fec70006a20 rank 0 nranks 1 cudaDev 1 busId 1c000 - Init COMPLETE
[2022-01-18 01:47:37,202] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 16126
[2022-01-18 01:47:37,202] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 16127
[2022-01-18 01:47:37,202] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 16128
[2022-01-18 01:47:37,202] [ERROR] [launch.py:137:sigkill_handler] ['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=2', '--block-lm', '--task-mask', '--bert-prob', '0.4', '--gap-sentence-prob', '0.3', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-large-chinese', '--model-parallel-size', '1', '--num-layers', '24', '--hidden-size', '1024', '--num-attention-heads', '16', '--seq-length', '512', '--max-position-embeddings', '1024', '--save', '../model_save/checkpoints/', '--load', '../model_save/checkpoints/', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000000', '--train-data', 'wudao', '--resume-dataloader', '--loader-scatter', '4', '--no-lazy-loader', '--tokenizer-type', 'ChineseSPTokenizer', '--fix-command-token', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '200000', '--warmup', '0.04', '--checkpoint-activations', '--deepspeed-activation-checkpointing', '--fp16', '--deepspeed', '--deepspeed_config', './config/config_block_large_chinese.json'] exits with return code = 1
