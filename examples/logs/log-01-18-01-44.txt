[2022-01-18 01:44:26,982] [INFO] [runner.py:327:main] Using IP address of 172.31.32.40 for node V100-3
[2022-01-18 01:44:26,982] [INFO] [runner.py:398:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJWMTAwLTMiOiBbMCwgMSwgMl19 --master_addr=172.31.32.40 --master_port=28800 pretrain_glm.py --block-lm --task-mask --bert-prob 0.4 --gap-sentence-prob 0.3 --avg-block-length 3 --gpt-min-ratio 0.25 --block-mask-prob 0.1 --short-seq-prob 0.02 --experiment-name blocklm-large-chinese --model-parallel-size 1 --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --seq-length 512 --max-position-embeddings 1024 --save ../model_save/checkpoints/ --load ../model_save/checkpoints/ --log-interval 50 --eval-interval 1000 --save-interval 2000 --train-iters 250000000 --train-data wudao --resume-dataloader --loader-scatter 4 --no-lazy-loader --tokenizer-type ChineseSPTokenizer --fix-command-token --split 949,50,1 --distributed-backend nccl --lr-decay-style cosine --lr-decay-ratio 0.1 --lr-decay-iters 200000 --warmup 0.04 --checkpoint-activations --deepspeed-activation-checkpointing --fp16 --deepspeed --deepspeed_config ./config/config_block_large_chinese.json
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NCCL_DEBUG=info
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL=2
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE=0
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda10.2
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NCCL_INCLUDE_DIR=/usr/include
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_VERSION=2.11.4-1
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda10.2
[2022-01-18 01:44:27,722] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2022-01-18 01:44:27,723] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2022-01-18 01:44:27,723] [INFO] [launch.py:73:main] 0 NCCL_LIBRARY=/usr/lib/x86_64-linux-gnu
[2022-01-18 01:44:27,723] [INFO] [launch.py:80:main] WORLD INFO DICT: {'V100-3': [0, 1, 2]}
[2022-01-18 01:44:27,723] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=3, node_rank=0
[2022-01-18 01:44:27,723] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'V100-3': [0, 1, 2]})
[2022-01-18 01:44:27,723] [INFO] [launch.py:100:main] dist_world_size=3
[2022-01-18 01:44:27,723] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0,1,2
Traceback (most recent call last):
  File "pretrain_glm.py", line 27, in <module>
    from arguments import get_args
  File "/data/wang/models/wudao/examples/glm/arguments.py", line 23, in <module>
    from utils.utils import get_hostname
ModuleNotFoundError: No module named 'utils'
Traceback (most recent call last):
  File "pretrain_glm.py", line 27, in <module>
    from arguments import get_args
  File "/data/wang/models/wudao/examples/glm/arguments.py", line 23, in <module>
    from utils.utils import get_hostname
ModuleNotFoundError: No module named 'utils'
Traceback (most recent call last):
  File "pretrain_glm.py", line 27, in <module>
    from arguments import get_args
  File "/data/wang/models/wudao/examples/glm/arguments.py", line 23, in <module>
    from utils.utils import get_hostname
ModuleNotFoundError: No module named 'utils'
[2022-01-18 01:44:28,748] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 16025
[2022-01-18 01:44:28,748] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 16026
[2022-01-18 01:44:28,749] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 16027
[2022-01-18 01:44:28,749] [ERROR] [launch.py:137:sigkill_handler] ['/opt/conda/bin/python', '-u', 'pretrain_glm.py', '--local_rank=2', '--block-lm', '--task-mask', '--bert-prob', '0.4', '--gap-sentence-prob', '0.3', '--avg-block-length', '3', '--gpt-min-ratio', '0.25', '--block-mask-prob', '0.1', '--short-seq-prob', '0.02', '--experiment-name', 'blocklm-large-chinese', '--model-parallel-size', '1', '--num-layers', '24', '--hidden-size', '1024', '--num-attention-heads', '16', '--seq-length', '512', '--max-position-embeddings', '1024', '--save', '../model_save/checkpoints/', '--load', '../model_save/checkpoints/', '--log-interval', '50', '--eval-interval', '1000', '--save-interval', '2000', '--train-iters', '250000000', '--train-data', 'wudao', '--resume-dataloader', '--loader-scatter', '4', '--no-lazy-loader', '--tokenizer-type', 'ChineseSPTokenizer', '--fix-command-token', '--split', '949,50,1', '--distributed-backend', 'nccl', '--lr-decay-style', 'cosine', '--lr-decay-ratio', '0.1', '--lr-decay-iters', '200000', '--warmup', '0.04', '--checkpoint-activations', '--deepspeed-activation-checkpointing', '--fp16', '--deepspeed', '--deepspeed_config', './config/config_block_large_chinese.json'] exits with return code = 1
