{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a1ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='/Users/guoqiang/Documents/wang/research/bert/cpt_wudao'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0afba",
   "metadata": {},
   "source": [
    "# albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f48b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁i', '▁like', '▁this', '▁movie', '▁app', 'rent', 'ly', '.']\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3]\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3]\n",
      "i like this movie apprently.\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3, 39, 25, 1632, 187, 3]\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3, 39, 25, 1632, 187, 3]\n",
      "i like this movie apprently. she is beautiful!\n",
      "{'input_ids': [2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3, 39, 25, 1632, 187, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_albert import AlbertTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AlbertTokenizer(root_dir+\"/data/vocab_data/albert/spiece.model\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3168ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35ae7f1e",
   "metadata": {},
   "source": [
    "# bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59790760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġlike', 'Ġthis', 'Ġmovie', 'Ġapp', 'rent', 'ly', '.']\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2]\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2]\n",
      "I like this movie apprently.\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2, 2, 2515, 16, 2721, 328, 2]\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2, 2, 2515, 16, 2721, 328, 2]\n",
      "I like this movie apprently.She is beautiful!\n",
      "{'input_ids': [0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2, 2, 2515, 16, 2721, 328, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_bart import BartTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BartTokenizer(root_dir+\"/data/vocab_data/roberta/roberta-base_vocab.json\", \n",
    "                              root_dir+\"/data/vocab_data/roberta/roberta-base_merges.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True)))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True)))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29fbe96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf185ed",
   "metadata": {},
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dceea1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'this', 'movie', 'app', '##rent', '##ly', '.']\n",
      "[101, 1045, 2066, 2023, 3185, 10439, 22787, 2135, 1012, 102]\n",
      "i like this movie apprently.\n",
      "[101, 1045, 2066, 2023, 3185, 10439, 22787, 2135, 1012, 102]\n",
      "[CLS] i like this movie apprently. [SEP]\n",
      "[101, 1045, 2066, 2023, 3185, 10439, 22787, 2135, 1012, 102, 2016, 2003, 3376, 999, 102]\n",
      "i like this movie apprently. she is beautiful!\n",
      "[101, 1045, 2066, 2023, 3185, 10439, 22787, 2135, 1012, 102, 2016, 2003, 3376, 999, 102]\n",
      "[CLS] i like this movie apprently. [SEP] she is beautiful! [SEP]\n",
      "{'input_ids': [101, 1045, 2066, 2023, 3185, 10439, 22787, 2135, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1045, 2066, 2023, 3185, 10439, 22787, 2135, 1012, 102, 2016, 2003, 3376, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['你', '吃', '饭', '了', '吗', '？']\n",
      "[101, 872, 1391, 7649, 749, 1408, 8043, 102]\n",
      "你吃饭了吗？\n",
      "[101, 872, 1391, 7649, 749, 1408, 8043, 102]\n",
      "[CLS]你吃饭了吗？[SEP]\n",
      "[101, 872, 1391, 7649, 749, 1408, 8043, 102, 2769, 3221, 704, 1744, 782, 511, 102]\n",
      "你吃饭了吗？我是中国人。\n",
      "[101, 872, 1391, 7649, 749, 1408, 8043, 102, 2769, 3221, 704, 1744, 782, 511, 102]\n",
      "[CLS]你吃饭了吗？[SEP]我是中国人。[SEP]\n",
      "{'input_ids': [101, 872, 1391, 7649, 749, 1408, 8043, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 872, 1391, 7649, 749, 1408, 8043, 102, 2769, 3221, 704, 1744, 782, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_bert import BertTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BertTokenizer(root_dir+\"/data/vocab_data/bert/bert-base-uncased_vocab.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True), skip_special_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True), skip_special_tokens=False))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))\n",
    "\n",
    "    tokenizer = BertTokenizer(root_dir+\"/data/vocab_data/bert/google_zh_vocab.txt\")\n",
    "    sentence = ['你吃饭了吗？', '我是中国人。']\n",
    "\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0]), spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True), skip_special_tokens=False, spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1]), spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True),\n",
    "                           skip_special_tokens=False, spaces_between_tokens=False))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c4451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66b6163",
   "metadata": {},
   "source": [
    "# char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7df5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'e', 't', \"'\", 's', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', '!', ' ', 'I', 's', 'n', \"'\", 't', ' ', 't', 'h', 'i', 's', ' ', 'e', 'a', 's', 'y', '?']\n",
      "['I', '[UNK]', 'l', 'i', 'k', 'e', '[UNK]', 't', 'h', 'i', 's', '[UNK]', 'm', 'o', 'v', 'i', 'e', '[UNK]', 'a', 'p', 'p', 'r', 'e', 'n', 't', 'l', 'y', '.']\n",
      "[146, 100, 181, 178, 180, 174, 100, 189, 177, 178, 188, 100, 182, 184, 191, 178, 174, 100, 170, 185, 185, 187, 174, 183, 189, 181, 194, 119]\n",
      "I[UNK]like[UNK]this[UNK]movie[UNK]apprently.\n",
      "[146, 100, 181, 178, 180, 174, 100, 189, 177, 178, 188, 100, 182, 184, 191, 178, 174, 100, 170, 185, 185, 187, 174, 183, 189, 181, 194, 119, 156, 177, 174, 100, 178, 188, 100, 171, 174, 170, 190, 189, 178, 175, 190, 181, 106]\n",
      "I[UNK]like[UNK]this[UNK]movie[UNK]apprently.She[UNK]is[UNK]beautiful!\n",
      "{'input_ids': [146, 100, 181, 178, 180, 174, 100, 189, 177, 178, 188, 100, 182, 184, 191, 178, 174, 100, 170, 185, 185, 187, 174, 183, 189, 181, 194, 119]}\n",
      "{'input_ids': [146, 100, 181, 178, 180, 174, 100, 189, 177, 178, 188, 100, 182, 184, 191, 178, 174, 100, 170, 185, 185, 187, 174, 183, 189, 181, 194, 119, 156, 177, 174, 100, 178, 188, 100, 171, 174, 170, 190, 189, 178, 175, 190, 181, 106]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_char import CharTokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = CharTokenizer()\n",
    "    text = \"Let's tokenize! Isn't this easy?\"\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "    tokenizer = CharTokenizer(root_dir+\"/data/vocab_data/bert/bert-base-cased_vocab.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0]), spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1]), spaces_between_tokens=False))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31706eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7631ab12",
   "metadata": {},
   "source": [
    "# gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264c5f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġlike', 'Ġthis', 'Ġmovie', 'Ġapp', 'rent', 'ly', '.']\n",
      "[40, 588, 428, 3807, 598, 1156, 306, 13]\n",
      "I like this movie apprently.\n",
      "{'input_ids': [40, 588, 428, 3807, 598, 1156, 306, 13], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = GPT2Tokenizer(root_dir+\"/data/vocab_data/gpt2/gpt2_vocab.json\",\\\n",
    "                              root_dir+\"/data/vocab_data/gpt2/gpt2_merges.txt\")\n",
    "    sentence = 'I like this movie apprently.'\n",
    "    print(tokenizer.tokenize(sentence))\n",
    "    print(tokenizer.encode(sentence))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence)))\n",
    "    print(tokenizer.encode_plus(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bab88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54495556",
   "metadata": {},
   "source": [
    "# roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f725a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġlike', 'Ġthis', 'Ġmovie', 'Ġapp', 'rent', 'ly', '.']\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2]\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2]\n",
      "I like this movie apprently.\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2, 2, 2515, 16, 2721, 328, 2]\n",
      "[0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2, 2, 2515, 16, 2721, 328, 2]\n",
      "I like this movie apprently.She is beautiful!\n",
      "{'input_ids': [0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [0, 100, 101, 42, 1569, 1553, 9854, 352, 4, 2, 2, 2515, 16, 2721, 328, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_roberta import RobertaTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = RobertaTokenizer(root_dir+\"/data/vocab_data/roberta/roberta-base_vocab.json\", \n",
    "                                 root_dir+\"/data/vocab_data/roberta/roberta-base_merges.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True)))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True)))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1439ac4",
   "metadata": {},
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b9fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'tokenize!', \"Isn't\", 'this', 'easy', '?']\n",
      "['I', 'like', 'this', 'movie', '[UNK]', '.']\n",
      "[146, 1176, 1142, 2523, 100, 119]\n",
      "I like this movie [UNK].\n",
      "[146, 1176, 1142, 2523, 100, 119, 1153, 1110, 2712, 106]\n",
      "I like this movie [UNK]. She is beautiful!\n",
      "{'input_ids': [146, 1176, 1142, 2523, 100, 119]}\n",
      "{'input_ids': [146, 1176, 1142, 2523, 100, 119, 1153, 1110, 2712, 106]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_space import SpaceTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = SpaceTokenizer()\n",
    "    text = \"Let's tokenize! Isn't this easy ?\"\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "    tokenizer = SpaceTokenizer(root_dir+\"/data/vocab_data/bert/bert-base-cased_vocab.txt\")\n",
    "    sentence = ['I like this movie apprently .', 'She is beautiful !']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b5ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3cea50f",
   "metadata": {},
   "source": [
    "# xlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b861fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁like', '▁this', '▁movie', '▁app', 'rent', 'ly', '.']\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 3]\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 3]\n",
      "I like this movie apprently.\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 147, 27, 1848, 136, 4, 3]\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 147, 27, 1848, 136, 4, 3]\n",
      "I like this movie apprently. She is beautiful!\n",
      "{'input_ids': [35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 147, 27, 1848, 136, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_xlnet import XLNetTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = XLNetTokenizer(root_dir+\"/data/vocab_data/xlnet/spiece.model\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db96cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83bcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7597c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd1630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
