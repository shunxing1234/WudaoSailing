{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a1ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='/Users/guoqiang/Documents/wang/research/bert/cpt_wudao'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0afba",
   "metadata": {},
   "source": [
    "# albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f48b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁i', '▁like', '▁this', '▁movie', '▁app', 'rent', 'ly', '.']\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3]\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3]\n",
      "i like this movie apprently.\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3, 39, 25, 1632, 187, 3]\n",
      "[2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3, 39, 25, 1632, 187, 3]\n",
      "i like this movie apprently. she is beautiful!\n",
      "{'input_ids': [2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [2, 31, 101, 48, 1308, 4865, 8993, 102, 9, 3, 39, 25, 1632, 187, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_albert import AlbertTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AlbertTokenizer(root_dir+\"/data/vocab_data/albert/spiece.model\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3168ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35ae7f1e",
   "metadata": {},
   "source": [
    "# bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59790760",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vocabs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1174dc4a1a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_bart.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_roberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerges_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_roberta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     def __init__(\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_gpt2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_bpe_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteBPEVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/vocabs/byte_bpe_vocab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbytes_to_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vocabs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_bart import BartTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BartTokenizer(root_dir+\"/data/vocab_data/roberta/roberta-base_vocab.json\", \"../data/vocab_data/roberta/roberta-base_merges.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True)))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True)))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29fbe96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf185ed",
   "metadata": {},
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dceea1fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find a vocabulary file at path '../data/vocab_data/bert/bert-base-uncased_vocab.txt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0984e1ca10c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/vocab_data/bert/bert-base-uncased_vocab.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I like this movie apprently.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'She is beautiful!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, tokenize_chinese_chars, strip_accents, unk_token, cls_token, sep_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find a vocabulary file at path '{vocab_file}'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find a vocabulary file at path '../data/vocab_data/bert/bert-base-uncased_vocab.txt'."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_bert import BertTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = BertTokenizer(\"../data/vocab_data/bert/bert-base-uncased_vocab.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True), skip_special_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True), skip_special_tokens=False))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))\n",
    "\n",
    "    tokenizer = BertTokenizer(\"../data/vocab_data/bert/google_zh_vocab.txt\")\n",
    "    sentence = ['你吃饭了吗？', '我是中国人。']\n",
    "\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0]), spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True), skip_special_tokens=False, spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1]), spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True),\n",
    "                           skip_special_tokens=False, spaces_between_tokens=False))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c4451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66b6163",
   "metadata": {},
   "source": [
    "# char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7df5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'e', 't', \"'\", 's', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', '!', ' ', 'I', 's', 'n', \"'\", 't', ' ', 't', 'h', 'i', 's', ' ', 'e', 'a', 's', 'y', '?']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/vocab_data/bert/bert-base-cased_vocab.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b06662acef6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/vocab_data/bert/bert-base-cased_vocab.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I like this movie apprently.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'She is beautiful!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_char.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_path, unk_token)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/vocabs/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVocabBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/guoqiang/Documents/wang/research/bert/cpt_wudao/data/vocab_data/google_zh_vocab.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/vocabs/vocab.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(self, vocab_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/guoqiang/Documents/wang/research/bert/cpt_wudao/data/vocab_data/google_zh_vocab.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mw2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/vocab_data/bert/bert-base-cased_vocab.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_char import CharTokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = CharTokenizer()\n",
    "    text = \"Let's tokenize! Isn't this easy?\"\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "    tokenizer = CharTokenizer(\"../data/vocab_data/bert/bert-base-cased_vocab.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0]), spaces_between_tokens=False))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1]), spaces_between_tokens=False))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31706eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7631ab12",
   "metadata": {},
   "source": [
    "# gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264c5f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vocabs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce58d5241834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_gpt2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_bpe_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteBPEVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/vocabs/byte_bpe_vocab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbytes_to_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vocabs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = GPT2Tokenizer(\"../data/vocab_data/gpt2/gpt2_vocab.json\", \"../data/vocab_data/gpt2/gpt2_merges.txt\")\n",
    "    sentence = 'I like this movie apprently.'\n",
    "    print(tokenizer.tokenize(sentence))\n",
    "    print(tokenizer.encode(sentence))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence)))\n",
    "    print(tokenizer.encode_plus(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bab88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54495556",
   "metadata": {},
   "source": [
    "# roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f725a28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vocabs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-24be9ae42b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_roberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_roberta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     def __init__(\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/tokenizations/tokenization_gpt2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_bpe_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mByteBPEVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/wang/research/bert/cpt_wudao/data_utils/vocabs/byte_bpe_vocab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbytes_to_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vocabs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_roberta import RobertaTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = RobertaTokenizer(\"../data/vocab_data/roberta/roberta-base_vocab.json\", \"../data/vocab_data/roberta/roberta-base_merges.txt\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], add_special_tokens=True)))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True)))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118a78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1439ac4",
   "metadata": {},
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11b9fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'tokenize!', \"Isn't\", 'this', 'easy', '?']\n",
      "['I', 'like', 'this', 'movie', '[UNK]', '.']\n",
      "[146, 1176, 1142, 2523, 100, 119]\n",
      "I like this movie [UNK].\n",
      "[146, 1176, 1142, 2523, 100, 119, 1153, 1110, 2712, 106]\n",
      "I like this movie [UNK]. She is beautiful!\n",
      "{'input_ids': [146, 1176, 1142, 2523, 100, 119]}\n",
      "{'input_ids': [146, 1176, 1142, 2523, 100, 119, 1153, 1110, 2712, 106]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_space import SpaceTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = SpaceTokenizer()\n",
    "    text = \"Let's tokenize! Isn't this easy ?\"\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "    tokenizer = SpaceTokenizer(\"../data/vocab_data/bert/bert-base-cased_vocab.txt\")\n",
    "    sentence = ['I like this movie apprently .', 'She is beautiful !']\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b5ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3cea50f",
   "metadata": {},
   "source": [
    "# xlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b861fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', '▁like', '▁this', '▁movie', '▁app', 'rent', 'ly', '.']\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 3]\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 3]\n",
      "I like this movie apprently.\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 147, 27, 1848, 136, 4, 3]\n",
      "[35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 147, 27, 1848, 136, 4, 3]\n",
      "I like this movie apprently. She is beautiful!\n",
      "{'input_ids': [35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [35, 115, 52, 1432, 5523, 9663, 111, 9, 4, 147, 27, 1848, 136, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "from data_utils.tokenizations.tokenization_xlnet import XLNetTokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = XLNetTokenizer(\"../data/vocab_data/xlnet/spiece.model\")\n",
    "    sentence = ['I like this movie apprently.', 'She is beautiful!']\n",
    "\n",
    "    print(tokenizer.tokenize(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0]))\n",
    "    print(tokenizer.encode(sentence[0], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0])))\n",
    "\n",
    "    print(tokenizer.encode(sentence[0], sentence[1]))\n",
    "    print(tokenizer.encode(sentence[0], sentence[1], add_special_tokens=True))\n",
    "    print(tokenizer.decode(tokenizer.encode(sentence[0], sentence[1])))\n",
    "\n",
    "    print(tokenizer.encode_plus(sentence[0]))\n",
    "    print(tokenizer.encode_plus(sentence[0], sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db96cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83bcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7597c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd1630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
