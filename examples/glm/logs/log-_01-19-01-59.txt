[2022-01-19 01:59:36,584] [WARNING] [runner.py:132:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2022-01-19 01:59:36,621] [INFO] [runner.py:398:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12228 finetune_glm.py --deepspeed --deepspeed_config config_tasks/config_blocklm_10B.json --finetune --cloze-eval --experiment-name _01-19-01-59 --task --data-dir --save /data/wang/models/model_save/checkpoints --seq-length --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --num-workers 1 --no-load-optim --no-load-lr-scheduler --pattern-id 0 --fp16 --model-parallel-size 1 --epochs --overwrite
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NCCL_DEBUG=info
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL=2
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE=0
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.11.4-1+cuda10.2
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NCCL_INCLUDE_DIR=/usr/include
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.11.4-1
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_VERSION=2.11.4-1
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.11.4-1
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.11.4-1+cuda10.2
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2022-01-19 01:59:37,367] [INFO] [launch.py:73:main] 0 NCCL_LIBRARY=/usr/lib/x86_64-linux-gnu
[2022-01-19 01:59:37,367] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2022-01-19 01:59:37,367] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=4, node_rank=0
[2022-01-19 01:59:37,367] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2022-01-19 01:59:37,367] [INFO] [launch.py:100:main] dist_world_size=4
[2022-01-19 01:59:37,367] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
usage: finetune_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE]
                       [--intermediate-size INTERMEDIATE_SIZE]
                       [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--output-dropout OUTPUT_DROPOUT]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--vocab-size VOCAB_SIZE] [--deep-init]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--cpu-optimizer] [--cpu_torch_adam] [--fp16]
                       [--fp32-embedding] [--fp32-layernorm]
                       [--fp32-tokentypes] [--fp32-allreduce]
                       [--hysteresis HYSTERESIS] [--loss-scale LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--min-scale MIN_SCALE]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE]
                       [--gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS]
                       [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--deepspeed-activation-checkpointing]
                       [--epochs EPOCHS] [--clip-grad CLIP_GRAD]
                       [--train-iters TRAIN_ITERS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--log-interval LOG_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--switch-linear] [--save SAVE]
                       [--new-save-directory] [--save-epoch SAVE_EPOCH]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--no-load-lr-scheduler]
                       [--no-deepspeed-load] [--finetune]
                       [--resume-dataloader]
                       [--distributed-backend {nccl,gloo}]
                       [--DDP-impl {local,torch,none}]
                       [--local_rank LOCAL_RANK] [--block-lm] [--masked-lm]
                       [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--eval-max-preds-per-seq EVAL_MAX_PREDS_PER_SEQ]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--out-seq-length OUT_SEQ_LENGTH]
                       [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE] [--shuffle]
                       [--filter-english]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--data-dir DATA_DIR]
                       [--input-data-sizes-file INPUT_DATA_SIZES_FILE]
                       [--delim DELIM] [--text-key TEXT_KEY]
                       [--eval-text-key EVAL_TEXT_KEY] [--split SPLIT]
                       [--no-lazy-loader] [--half-lazy-loader]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--num-workers NUM_WORKERS]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-path TOKENIZER_PATH]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--fix-command-token] [--no-pre-tokenize]
                       [--cache-dir CACHE_DIR] [--use-tfrecords]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--max-preds-per-seq MAX_PREDS_PER_SEQ]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--task TASK]
                       [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--few-superglue] [--eval-valid]
                       [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--deepspeed] [--deepspeed_config DEEPSPEED_CONFIG]
                       [--deepscale] [--deepscale_config DEEPSCALE_CONFIG]
                       [--deepspeed_mpi]
finetune_glm.py: error: argument --task: expected one argument
usage: finetune_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE]
                       [--intermediate-size INTERMEDIATE_SIZE]
                       [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--output-dropout OUTPUT_DROPOUT]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--vocab-size VOCAB_SIZE] [--deep-init]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--cpu-optimizer] [--cpu_torch_adam] [--fp16]
                       [--fp32-embedding] [--fp32-layernorm]
                       [--fp32-tokentypes] [--fp32-allreduce]
                       [--hysteresis HYSTERESIS] [--loss-scale LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--min-scale MIN_SCALE]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE]
                       [--gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS]
                       [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--deepspeed-activation-checkpointing]
                       [--epochs EPOCHS] [--clip-grad CLIP_GRAD]
                       [--train-iters TRAIN_ITERS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--log-interval LOG_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--switch-linear] [--save SAVE]
                       [--new-save-directory] [--save-epoch SAVE_EPOCH]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--no-load-lr-scheduler]
                       [--no-deepspeed-load] [--finetune]
                       [--resume-dataloader]
                       [--distributed-backend {nccl,gloo}]
                       [--DDP-impl {local,torch,none}]
                       [--local_rank LOCAL_RANK] [--block-lm] [--masked-lm]
                       [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--eval-max-preds-per-seq EVAL_MAX_PREDS_PER_SEQ]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--out-seq-length OUT_SEQ_LENGTH]
                       [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE] [--shuffle]
                       [--filter-english]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--data-dir DATA_DIR]
                       [--input-data-sizes-file INPUT_DATA_SIZES_FILE]
                       [--delim DELIM] [--text-key TEXT_KEY]
                       [--eval-text-key EVAL_TEXT_KEY] [--split SPLIT]
                       [--no-lazy-loader] [--half-lazy-loader]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--num-workers NUM_WORKERS]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-path TOKENIZER_PATH]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--fix-command-token] [--no-pre-tokenize]
                       [--cache-dir CACHE_DIR] [--use-tfrecords]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--max-preds-per-seq MAX_PREDS_PER_SEQ]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--task TASK]
                       [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--few-superglue] [--eval-valid]
                       [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--deepspeed] [--deepspeed_config DEEPSPEED_CONFIG]
                       [--deepscale] [--deepscale_config DEEPSCALE_CONFIG]
                       [--deepspeed_mpi]
finetune_glm.py: error: argument --task: expected one argument
usage: finetune_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE]
                       [--intermediate-size INTERMEDIATE_SIZE]
                       [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--output-dropout OUTPUT_DROPOUT]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--vocab-size VOCAB_SIZE] [--deep-init]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--cpu-optimizer] [--cpu_torch_adam] [--fp16]
                       [--fp32-embedding] [--fp32-layernorm]
                       [--fp32-tokentypes] [--fp32-allreduce]
                       [--hysteresis HYSTERESIS] [--loss-scale LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--min-scale MIN_SCALE]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE]
                       [--gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS]
                       [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--deepspeed-activation-checkpointing]
                       [--epochs EPOCHS] [--clip-grad CLIP_GRAD]
                       [--train-iters TRAIN_ITERS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--log-interval LOG_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--switch-linear] [--save SAVE]
                       [--new-save-directory] [--save-epoch SAVE_EPOCH]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--no-load-lr-scheduler]
                       [--no-deepspeed-load] [--finetune]
                       [--resume-dataloader]
                       [--distributed-backend {nccl,gloo}]
                       [--DDP-impl {local,torch,none}]
                       [--local_rank LOCAL_RANK] [--block-lm] [--masked-lm]
                       [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--eval-max-preds-per-seq EVAL_MAX_PREDS_PER_SEQ]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--out-seq-length OUT_SEQ_LENGTH]
                       [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE] [--shuffle]
                       [--filter-english]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--data-dir DATA_DIR]
                       [--input-data-sizes-file INPUT_DATA_SIZES_FILE]
                       [--delim DELIM] [--text-key TEXT_KEY]
                       [--eval-text-key EVAL_TEXT_KEY] [--split SPLIT]
                       [--no-lazy-loader] [--half-lazy-loader]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--num-workers NUM_WORKERS]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-path TOKENIZER_PATH]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--fix-command-token] [--no-pre-tokenize]
                       [--cache-dir CACHE_DIR] [--use-tfrecords]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--max-preds-per-seq MAX_PREDS_PER_SEQ]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--task TASK]
                       [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--few-superglue] [--eval-valid]
                       [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--deepspeed] [--deepspeed_config DEEPSPEED_CONFIG]
                       [--deepscale] [--deepscale_config DEEPSCALE_CONFIG]
                       [--deepspeed_mpi]
finetune_glm.py: error: argument --task: expected one argument
usage: finetune_glm.py [-h] [--transformer-xl] [--pretrained-bert]
                       [--encoder-decoder]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--hidden-size HIDDEN_SIZE]
                       [--intermediate-size INTERMEDIATE_SIZE]
                       [--num-layers NUM_LAYERS]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--output-dropout OUTPUT_DROPOUT]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--vocab-size VOCAB_SIZE] [--deep-init]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--cpu-optimizer] [--cpu_torch_adam] [--fp16]
                       [--fp32-embedding] [--fp32-layernorm]
                       [--fp32-tokentypes] [--fp32-allreduce]
                       [--hysteresis HYSTERESIS] [--loss-scale LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--min-scale MIN_SCALE]
                       [--attention-scale ATTENTION_SCALE]
                       [--experiment-name EXPERIMENT_NAME]
                       [--batch-size BATCH_SIZE]
                       [--gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS]
                       [--weight-decay WEIGHT_DECAY]
                       [--checkpoint-activations]
                       [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
                       [--deepspeed-activation-checkpointing]
                       [--epochs EPOCHS] [--clip-grad CLIP_GRAD]
                       [--train-iters TRAIN_ITERS]
                       [--label-smoothing LABEL_SMOOTHING]
                       [--log-interval LOG_INTERVAL]
                       [--summary-dir SUMMARY_DIR] [--seed SEED]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-style {constant,linear,cosine,exponential}]
                       [--lr-decay-ratio LR_DECAY_RATIO] [--lr LR]
                       [--warmup WARMUP] [--switch-linear] [--save SAVE]
                       [--new-save-directory] [--save-epoch SAVE_EPOCH]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--no-load-lr-scheduler]
                       [--no-deepspeed-load] [--finetune]
                       [--resume-dataloader]
                       [--distributed-backend {nccl,gloo}]
                       [--DDP-impl {local,torch,none}]
                       [--local_rank LOCAL_RANK] [--block-lm] [--masked-lm]
                       [--bert-prob BERT_PROB]
                       [--gpt-infill-prob GPT_INFILL_PROB]
                       [--gpt-min-ratio GPT_MIN_RATIO]
                       [--gap-sentence-prob GAP_SENTENCE_PROB]
                       [--gap-sentence-ratio GAP_SENTENCE_RATIO]
                       [--avg-block-length AVG_BLOCK_LENGTH]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--single-span-prob SINGLE_SPAN_PROB] [--task-mask]
                       [--no-shuffle-block] [--no-block-position]
                       [--sentinel-token] [--block-mask-prob BLOCK_MASK_PROB]
                       [--context-mask-ratio CONTEXT_MASK_RATIO]
                       [--random-position] [--eval-batch-size EVAL_BATCH_SIZE]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--eval-epoch EVAL_EPOCH]
                       [--eval-seq-length EVAL_SEQ_LENGTH]
                       [--eval-max-preds-per-seq EVAL_MAX_PREDS_PER_SEQ]
                       [--overlapping-eval OVERLAPPING_EVAL]
                       [--temperature TEMPERATURE] [--top_p TOP_P]
                       [--top_k TOP_K] [--out-seq-length OUT_SEQ_LENGTH]
                       [--num-beams NUM_BEAMS]
                       [--length-penalty LENGTH_PENALTY]
                       [--no-repeat-ngram-size NO_REPEAT_NGRAM_SIZE]
                       [--min-tgt-length MIN_TGT_LENGTH] [--select-topk]
                       [--blank-maskratio BLANK_MASKRATIO]
                       [--model-parallel-size MODEL_PARALLEL_SIZE] [--shuffle]
                       [--filter-english]
                       [--train-data TRAIN_DATA [TRAIN_DATA ...]]
                       [--valid-data [VALID_DATA [VALID_DATA ...]]]
                       [--test-data [TEST_DATA [TEST_DATA ...]]]
                       [--data-dir DATA_DIR]
                       [--input-data-sizes-file INPUT_DATA_SIZES_FILE]
                       [--delim DELIM] [--text-key TEXT_KEY]
                       [--eval-text-key EVAL_TEXT_KEY] [--split SPLIT]
                       [--no-lazy-loader] [--half-lazy-loader]
                       [--loader-scatter LOADER_SCATTER] [--loose-json]
                       [--presplit-sentences] [--num-workers NUM_WORKERS]
                       [--tokenizer-model-type TOKENIZER_MODEL_TYPE]
                       [--tokenizer-path TOKENIZER_PATH]
                       [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer,BertWordPieceTokenizer,GPT2BPETokenizer,ChineseSPTokenizer}]
                       [--fix-command-token] [--no-pre-tokenize]
                       [--cache-dir CACHE_DIR] [--use-tfrecords]
                       [--seq-length SEQ_LENGTH] [--mem-length MEM_LENGTH]
                       [--max-preds-per-seq MAX_PREDS_PER_SEQ]
                       [--non-sentence-start NON_SENTENCE_START]
                       [--sample-one-document] [--load-splits LOAD_SPLITS]
                       [--save-splits SAVE_SPLITS]
                       [--save-test-data SAVE_TEST_DATA]
                       [--multi-task-data [MULTI_TASK_DATA [MULTI_TASK_DATA ...]]]
                       [--multi-task-ratio MULTI_TASK_RATIO]
                       [--multi-seq-length MULTI_SEQ_LENGTH]
                       [--multi-batch-size MULTI_BATCH_SIZE] [--task TASK]
                       [--load-pretrained LOAD_PRETRAINED]
                       [--pool-token {start,pad,cls}] [--cloze-eval]
                       [--multi-token] [--segment-length SEGMENT_LENGTH]
                       [--loss-func {cross_entropy,hinge,generative,mix}]
                       [--block-lm-ratio BLOCK_LM_RATIO] [--adapet]
                       [--pattern-id PATTERN_ID] [--fast-decode]
                       [--few-superglue] [--eval-valid]
                       [--validation-metric VALIDATION_METRIC]
                       [--unidirectional] [--src-seq-length SRC_SEQ_LENGTH]
                       [--tgt-seq-length TGT_SEQ_LENGTH]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--optimizer {adam,adafactor}]
                       [--wsc-negative] [--overwrite] [--no-validation]
                       [--continuous-prompt]
                       [--num-prompt-tokens NUM_PROMPT_TOKENS]
                       [--prompt-func {lstm,mlp,none}] [--freeze-transformer]
                       [--tune-prefix-layers TUNE_PREFIX_LAYERS]
                       [--prefix-prompt PREFIX_PROMPT] [--prompt-init]
                       [--deepspeed] [--deepspeed_config DEEPSPEED_CONFIG]
                       [--deepscale] [--deepscale_config DEEPSCALE_CONFIG]
                       [--deepspeed_mpi]
finetune_glm.py: error: argument --task: expected one argument
[2022-01-19 01:59:40,396] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 45432
[2022-01-19 01:59:40,396] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 45433
[2022-01-19 01:59:40,396] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 45434
[2022-01-19 01:59:40,396] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 45435
[2022-01-19 01:59:40,396] [ERROR] [launch.py:137:sigkill_handler] ['/opt/conda/bin/python', '-u', 'finetune_glm.py', '--local_rank=3', '--deepspeed', '--deepspeed_config', 'config_tasks/config_blocklm_10B.json', '--finetune', '--cloze-eval', '--experiment-name', '_01-19-01-59', '--task', '--data-dir', '--save', '/data/wang/models/model_save/checkpoints', '--seq-length', '--checkpoint-activations', '--eval-batch-size', '16', '--save-epoch', '100000', '--num-workers', '1', '--no-load-optim', '--no-load-lr-scheduler', '--pattern-id', '0', '--fp16', '--model-parallel-size', '1', '--epochs', '--overwrite'] exits with return code = 2
